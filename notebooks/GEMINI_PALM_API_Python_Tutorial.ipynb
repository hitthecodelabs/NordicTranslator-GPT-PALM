{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7326e2aa-f7a0-4c9d-9367-3f1d2818f183",
   "metadata": {},
   "source": [
    "# PALM & GEMINI API Python Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba265d4e-3d64-486a-88e2-07ec54ff74a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Installation\n",
    "\n",
    "!pip install \"shapely<2.0.0\"\n",
    "!pip install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297af9a1-41e1-40fe-9d89-55986d376d29",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Authenticating Google Account User in Google Colab\n",
    "\n",
    "When working with Google Colab for various tasks, including accessing and manipulating data from Google's services like Google Cloud Storage or BigQuery, it's essential to authenticate the user's Google account. The process of authentication has evolved to be more streamlined and secure. Here's how it benefits your workflow:\n",
    "\n",
    "#### Main Features of the Current Authentication Process:\n",
    "\n",
    "1. **Direct Integration:** By using `from google.colab import auth` followed by `auth.authenticate_user()`, you initiate a direct and secure authentication process. This method is built into Google Colab and is designed to seamlessly integrate with your Google account, providing a straightforward way for the notebook to gain the necessary permissions.\n",
    "\n",
    "2. **Enhanced Security:** The authentication process ensures that your data and resources are accessed securely. By authenticating, you're granting specific permissions to the notebook, which prevents unauthorized access to your Google services. This method does not require storing sensitive information in the notebook, thus enhancing overall security.\n",
    "\n",
    "3. **Ease of Use:** The current authentication approach eliminates the need for manual handling of environment variables or .env files. This means there's no need to manage separate configuration files or worry about exposing sensitive information. Users can authenticate directly within the notebook, making the process much more user-friendly and less error-prone.\n",
    "\n",
    "#### Implementing the Authentication:\n",
    "\n",
    "To authenticate your Google account in a Google Colab notebook, use the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8dab17-93e7-4483-97aa-baeecd04b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth as google_auth\n",
    "google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be1a7c-c63c-484e-8441-9bd5e9515deb",
   "metadata": {},
   "source": [
    "This script initiates the authentication process, prompting you to log in with your Google account. Once authenticated, the notebook will have the necessary permissions to access and interact with Google services under your account, following the scope and permissions you've granted. This streamlined process is essential for efficiently working with data and services in the Google ecosystem directly from your Google Colab notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b6877b-9975-4516-ade7-2abb1c1d00a1",
   "metadata": {},
   "source": [
    "### Initializing the Large Language Models: PaLM and Gemini\n",
    "\n",
    "When working with Google's Large Language Models like PaLM and Gemini, you'll be utilizing Google's Vertex AI platform. Similar to initializing a client with OpenAI, setting up your environment to interact with these advanced models is crucial. Here's how the initialization benefits you and how to do it:\n",
    "\n",
    "#### Importance of Proper Initialization:\n",
    "\n",
    "1. **Authentication and Access:** By initializing the client with your Google Cloud project details, you authenticate your access, ensuring that your requests are securely processed under your account. This step verifies your identity and authorizes you to use the models under Google's policies.\n",
    "\n",
    "2. **Billing and Resource Management:** Google Cloud operates on a usage-based pricing model. By initializing with your project details, you're linking your usage to your billing account, allowing for accurate tracking and management of your expenditures and quotas.\n",
    "\n",
    "3. **Customization and Configuration:** Initialization allows you to set parameters that control the behavior of the model. You can specify things like the number of candidates, token limits, and temperature, tailoring the model's responses to your specific needs.\n",
    "\n",
    "4. **Access to Advanced Features:** By initializing and specifying the particular model (like \"text-unicorn@001\" or \"gemini-pro\"), you gain access to the unique features and capabilities of each model, allowing you to leverage the latest advancements in language processing for your tasks.\n",
    "\n",
    "#### Scripts for Initializing Google Models:\n",
    "\n",
    "##### For PaLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a31bdd-b736-48a0-94ae-3debc0fb3c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "# Initialize the Vertex AI with your project and location details\n",
    "vertexai.init(project=\"your-project-id\", location=\"us-central1\")\n",
    "\n",
    "# Set your desired parameters\n",
    "parameters = {\n",
    "    \"candidate_count\": 1,\n",
    "    ### 1024 max using text-unicorn@001\n",
    "    ### 2048 max using text-bison@002\n",
    "    \"max_output_tokens\": 1024,\n",
    "    \"temperature\": 1,\n",
    "    \"top_k\": 40\n",
    "}\n",
    "\n",
    "# Initialize the model with the specific version (text-bison@002, text-unicorn@001)\n",
    "model = TextGenerationModel.from_pretrained(\"text-unicorn@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426fab8a-ed30-41cd-9ba1-10a9cdeb2570",
   "metadata": {},
   "source": [
    "##### For Gemini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083d503-d8c6-4090-8392-706a9372eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "# Initialize the Vertex AI with your project and location details\n",
    "vertexai.init(project=\"your-project-id\", location=\"your-location\")\n",
    "\n",
    "# Create the Gemini model instance\n",
    "model = GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "# Set up your generation configuration\n",
    "generation_config = {\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_p\": 1\n",
    "}\n",
    "\n",
    "# Generate content using the model\n",
    "responses = model.generate_content(\n",
    "    'your_prompt',\n",
    "    generation_config=generation_config,\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce7f62-7cc6-4751-b91e-664f249815d3",
   "metadata": {},
   "source": [
    "Replace `\"your-project-id\"` and `\"your-location\"` with your actual Google Cloud project ID and the location where your Vertex AI resources are hosted. The parameters and model IDs can also be adjusted based on your specific requirements and the capabilities of the model versions you are using.\n",
    "\n",
    "By following these steps, you'll set up a robust environment for interacting with Google's powerful language models, enabling you to integrate advanced natural language processing capabilities into your applications and workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b75ce96-aaea-4017-bdd5-58b280378e9f",
   "metadata": {},
   "source": [
    "### Google Models Overview: PaLM and Gemini\n",
    "\n",
    "Google has developed a range of powerful foundation models under its Vertex AI platform, including PaLM and Gemini, each tailored for specific use cases and offering unique capabilities. Understanding these models and their features is key to leveraging their potential in various applications.\n",
    "\n",
    "#### Foundation Model APIs in Vertex AI:\n",
    "\n",
    "Vertex AI provides a suite of foundation model APIs, each designed for different tasks:\n",
    "\n",
    "- **Gemini API:** Handles multimodal data, text, code, and chat.\n",
    "- **PaLM API:** Specializes in text, chat, and embeddings.\n",
    "- **Codey APIs:** Focuses on code generation, code chat, and code completion.\n",
    "- **Imagen API:** Works with image generation, editing, captioning, visual question answering, and multimodal embedding.\n",
    "\n",
    "#### Gemini API Models:\n",
    "\n",
    "##### Preview:\n",
    "\n",
    "Gemini API is in Preview, subject to specific terms for pre-GA offerings. It's essential to note that these models might have limited support and could undergo significant changes.\n",
    "\n",
    "##### Models Available:\n",
    "\n",
    "1. **Gemini Pro (gemini-pro):** Designed for natural language tasks, multiturn text and code chat, and code generation. It's best for text-only prompts.\n",
    "    - Max tokens (input and output): 32,760\n",
    "    - Max output tokens: 8,192\n",
    "    - Training data: Up to Feb 2023\n",
    "2. **Gemini Pro Vision (gemini-pro-vision):** A multimodal model supporting text, image, and video inputs for text or code responses. Ideal for prompts involving multimedia.    \n",
    "    - Max tokens (input and output): 16,384\n",
    "    - Max output tokens: 2,048\n",
    "    - Max image size: No limit\n",
    "    - Max images per prompt: 16\n",
    "    - Max video length: 2 minutes\n",
    "    - Max videos per prompt: 1\n",
    "    - Training data: Up to Feb 2023\n",
    "\n",
    "#### PaLM API Models:\n",
    "\n",
    "The PaLM API offers models fine-tuned for a range of natural language tasks:\n",
    "\n",
    "\n",
    "1. **PaLM 2 for Text (text-bison):** Suitable for classification, summarization, extraction, and more.\n",
    "   - Maximum input tokens: 8192\n",
    "   - Maximum output tokens: 2048\n",
    "   - Training data: Up to Feb 2023\n",
    "   - Supervised: Yes\n",
    "   - RLHF (Reinforcement Learning from Human Feedback): Yes\n",
    "2. **PaLM 2 for Text (text-unicorn):** Advanced text model for complex natural language tasks.\n",
    "   - Maximum input tokens: 8192\n",
    "   - Maximum output tokens: 1024\n",
    "   - Training data: Up to Feb 2023\n",
    "   - Supervised: No\n",
    "   - RLHF: No\n",
    "   - Distillation: Yes (Preview)\n",
    "3. **PaLM 2 for Text 32k (text-bison-32k):** Designed for a variety of language tasks with a larger token limit.\n",
    "   - Max tokens (input + output): 32,768\n",
    "   - Max output tokens: 8,192\n",
    "   - Training data: Up to Aug 2023\n",
    "   - Supervised: No\n",
    "   - RLHF: No\n",
    "   \n",
    "\n",
    "When choosing the right model, consider your specific requirements, including the task complexity, desired capabilities, and budget. For more detailed information on each model, refer to the official [Vertex AI documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82115229-e9eb-412c-99ee-5b0bbf3f1fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " The Roman Empire was divided into two parts in 395 AD: the Western Roman Empire and the Eastern Roman Empire. The Western Roman Empire collapsed in 476 AD, but the Eastern Roman Empire continued to exist until 1453 AD."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "# Define your prompt\n",
    "prompt = \"Tell me a fact about the Roman Empire\"\n",
    "\n",
    "# Create a TextGenerationModel instance for the Bison model\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@002\")\n",
    "\n",
    "# Set your generation parameters\n",
    "generation_params = {\n",
    "    \"max_output_tokens\": 2048,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 1,\n",
    "    \"candidate_count\": 1\n",
    "}\n",
    "\n",
    "# Generate a response using the model\n",
    "response = model.predict(prompt, **generation_params)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc706b52-34c9-4fc0-a042-a089aeacb471",
   "metadata": {},
   "source": [
    "## Example Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f69d25-65fa-44ec-968c-d99cf7367db2",
   "metadata": {},
   "source": [
    "### 1. Creative Writing Assistance (Using Bison)\n",
    "\n",
    "#### Use Case:\n",
    "\n",
    "- Assisting writers in generating ideas, plot points, or even entire sections of text.\n",
    "- Ideal for scriptwriters, novelists, or content creators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa619e0f-a780-4d7b-9286-41a9dfbf8008",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " In a world ravaged by climate change and political corruption, humanity teeters on the brink of collapse. Cities have become sprawling wastelands, choked with smog and teeming with desperate survivors. The few remaining pockets of civilization are ruled by ruthless warlords, who maintain their power through fear and violence. In this bleak future, hope is a scarce commodity, and the struggle for survival is a daily battle."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering:\n",
    "\n",
    "prompt = \"Write a thrilling and suspenseful opening paragraph for a science fiction novel set in a dystopian future.\"\n",
    "\n",
    "# Create a TextGenerationModel instance for the Bison model\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@002\")\n",
    "\n",
    "# Set your generation parameters\n",
    "generation_params = {\n",
    "    \"max_output_tokens\": 2048,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 1,\n",
    "    \"candidate_count\": 1\n",
    "}\n",
    "\n",
    "# Generate a response using the model\n",
    "response = model.predict(prompt, **generation_params)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe9755c-6b72-4a1b-a539-8c493c3e862e",
   "metadata": {},
   "source": [
    "### 2. Code Debugging and Suggestions (Using Gemini-Pro)\n",
    "\n",
    "#### Use Case:\n",
    "\n",
    "- Helping programmers debug code or suggesting improvements to existing code.\n",
    "- Useful for software developers and coding enthusiasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4804046e-85c9-476b-91d1-ae37c8682f71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bug in the code is that it uses `number` instead of `numbers` in the line `count = len(number)`. This will result in a NameError because `number` is not defined.\n",
      "\n",
      "To fix this bug, you should use `numbers` instead of `number` in that line. The corrected code should look like this:\n",
      "\n",
      "```python\n",
      "def calculate_average(numbers:list):\n",
      "    total_sum = sum(numbers)\n",
      "    count = len(numbers)\n",
      "    average = total_sum / count\n",
      "    return average\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "### Prompt Engineering:\n",
    "\n",
    "prompt = \"\"\"Here is a Python function with a bug. Can you identify the bug and suggest a correction?\n",
    "\n",
    "def calculate_average(numbers:list):\n",
    "    total_sum = sum(numbers)\n",
    "    count = len(number)\n",
    "    average = total_sum / count\n",
    "    return average\"\"\"\n",
    "\n",
    "model = GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "generation_params = {\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_p\": 1\n",
    "}\n",
    "\n",
    "response = model.generate_content(prompt, generation_config=generation_params)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76325ff5-e029-45c0-aecc-10d55848f343",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Language Translation (Using Unicorn)\n",
    "\n",
    "#### Use Case:\n",
    "\n",
    "- Translating text from one language to another.\n",
    "- Useful for translators, international businesses, or travelers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdc42061-4ab5-41c6-9104-926ea99ca85f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Hei, hvordan har du det i dag?"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering:\n",
    "\n",
    "prompt = \"Translate the following text from English to Norwegian: 'Hello, how are you today?'\"\n",
    "\n",
    "# Create a TextGenerationModel instance for the Unicorn model\n",
    "model = TextGenerationModel.from_pretrained(\"text-unicorn@001\")\n",
    "\n",
    "# Set your generation parameters\n",
    "generation_params = {\n",
    "    \"max_output_tokens\": 1024,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 1,\n",
    "    \"candidate_count\": 1\n",
    "}\n",
    "\n",
    "# Generate a response using the model\n",
    "response = model.predict(prompt, **generation_params)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b7f0a-7f72-4d77-9586-499a5bf341f9",
   "metadata": {},
   "source": [
    "### 4. Educational Content Creation (Using Gemini-Pro)\n",
    "### Use Case:\n",
    "- Generating educational content or explanations on various topics.\n",
    "- Ideal for educators, students, or e-learning platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fe0874f-3188-4a3a-8112-913c69e51be8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Plants:** Green plants produce their own food through the process of photosynthesis.\n",
      "\n",
      "2. **Sunlight:** Plants have special cells called chloroplasts that contain a green pigment called chlorophyll. Chlorophyll captures light energy from the sun.\n",
      "\n",
      "3. **Water:** Plants absorb water through their roots and transport it to the leaves.\n",
      "\n",
      "4. **Carbon Dioxide:** Plants take in carbon dioxide from the air through small pores on their leaves called stomata.\n",
      "\n",
      "5. **Calvin Cycle:** Inside the chloroplasts, a series of chemical reactions called the Calvin cycle use the light energy captured by chlorophyll to convert water and carbon dioxide into glucose (a type of sugar).\n",
      "\n",
      "6. **Oxygen:** As a byproduct of photosynthesis, plants release oxygen into the air through the stomata.\n",
      "\n",
      "7. **Glucose Storage:** The glucose produced by photosynthesis is used as energy by the plant or stored as starch for later use.\n",
      "\n",
      "8. **Importance:** Photosynthesis is essential for plants to make their own food and provide oxygen for other living things in the ecosystem.\n"
     ]
    }
   ],
   "source": [
    "### Prompt Engineering:\n",
    "\n",
    "prompt = \"Explain the concept of photosynthesis in simple terms for a middle school science class.\"\n",
    "model = GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "generation_params = {\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_p\": 1\n",
    "}\n",
    "\n",
    "response = model.generate_content(prompt, generation_config=generation_params)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0d8a1-5d49-4505-b316-f493b054dc8e",
   "metadata": {},
   "source": [
    "*Each of these examples demonstrates how to tailor prompts to specific tasks and choose the most suitable model. Remember, the quality of the output heavily depends on how well the prompt is constructed and the context provided.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96633477-4d92-4377-b685-c8ba7fc626c0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial has provided a comprehensive overview of how to interact with large language models from Google using the Vertex API with Python. From setting up your environment and initializing the user and model to sending requests and interpreting responses, we've covered the fundamental aspects to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632301b-f837-43c6-b694-1bcc857d64c1",
   "metadata": {},
   "source": [
    "### Key Takeaways for PaLM and Gemini Models\n",
    "\n",
    "- **Environment Setup:** Emphasizing the importance of securely initializing your environment in Vertex AI with project details and authentication for secure and efficient model interaction.\n",
    "- **Understanding Google Models:** Gaining insights into Google's advanced models like PaLM and Gemini, their distinct capabilities, and how to select the most suitable one for your specific needs.\n",
    "- **Prompt Engineering:** The skill of creating effective prompts that are well-suited to the specificities of PaLM and Gemini models to obtain the most relevant and accurate responses.\n",
    "- **Interpreting Responses:** Strategies for analyzing and interpreting the outputs from PaLM and Gemini, understanding their context, and how they can be applied to your tasks.\n",
    "- **Best Practices:** Recommendations for prompt design, understanding the limitations and capabilities of each model, managing computation costs, and adhering to ethical guidelines in AI usage.\n",
    "- **Example Use Cases:** Real-world scenarios and examples demonstrating how PaLM and Gemini models can be applied across different domains and tasks.\n",
    "\n",
    "As you embark on using Google's foundation models, remember that the field of AI and machine learning is ever-evolving. Staying informed about the latest developments, experimenting with new features, and continuously learning will help you make the most out of these powerful tools.\n",
    "\n",
    "We hope this guide serves as a valuable starting point for your journey with PaLM and Gemini models, sparking ideas and providing the knowledge needed to implement these advanced technologies in your projects effectively.\n",
    "\n",
    "For more detailed information and continuous updates on Google's models, refer to the official documentation:\n",
    "\n",
    "- [Google Models on Vertex AI Documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models).\n",
    "- [Vertex AI Studio: Gallery](https://console.cloud.google.com/vertex-ai/generative/language/gallery).\n",
    "- [Vertex AI Studio: Text Generation](https://console.cloud.google.com/vertex-ai/generative/language/create/text)."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
